{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------  122.9/123.5 kB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 123.5/123.5 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp38-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp38-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.6/7.9 MB 49.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.9/7.9 MB 42.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.7/7.9 MB 43.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 42.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 311.7/311.7 kB ? eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp38-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 277.7/277.7 kB ? eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp38-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.3/2.2 MB 42.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 35.3 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 166.4/166.4 kB ? eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.4 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.2-cp38-cp38-win_amd64.whl.metadata (26 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp38-cp38-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp38-cp38-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp38-cp38-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy)\n",
      "  Downloading thinc-8.2.1-cp38-cp38-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp38-cp38-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "     ---------------------------------------- 0.0/65.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 65.2/65.2 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 181.6/181.6 kB 11.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.14.5-cp38-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Downloading blis-0.7.11-cp38-cp38-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Downloading spacy-3.7.2-cp38-cp38-win_amd64.whl (12.5 MB)\n",
      "   ---------------------------------------- 0.0/12.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/12.5 MB 49.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.3/12.5 MB 35.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.5/12.5 MB 43.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.8/12.5 MB 41.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.0/12.5 MB 45.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.5 MB 43.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.5/12.5 MB 40.9 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp38-cp38-win_amd64.whl (39 kB)\n",
      "Downloading murmurhash-1.0.10-cp38-cp38-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp38-cp38-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/123.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 123.0/123.0 kB ? eta 0:00:00\n",
      "Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "   ---------------------------------------- 0.0/381.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 381.9/381.9 kB ? eta 0:00:00\n",
      "Downloading pydantic_core-2.14.5-cp38-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.9/1.9 MB 59.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 39.9 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB ? eta 0:00:00\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp38-cp38-win_amd64.whl (483 kB)\n",
      "   ---------------------------------------- 0.0/483.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 483.7/483.7 kB ? eta 0:00:00\n",
      "Downloading thinc-8.2.1-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 48.3 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.1/6.6 MB 66.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 41.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.3/6.6 MB 42.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 35.2 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: cymem, wasabi, typing-extensions, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, catalogue, blis, typer, srsly, pydantic-core, preshed, cloudpathlib, annotated-types, pydantic, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.5.2 pydantic-core-2.14.5 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 typer-0.9.0 typing-extensions-4.8.0 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Input, Activation, Dense # control_dependencies,\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>pseudo_author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_likes</th>\n",
       "      <th>author_app_version</th>\n",
       "      <th>review_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7e73f80e-a8fd-4ff3-b09b-502f0ad058ff</td>\n",
       "      <td>152618553977019693742</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Works great on my Evo! Glad android phones are...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2.0 build 819145-1.2.0-102</td>\n",
       "      <td>2011-05-12 18:50:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dab55eca-c2a0-45a8-93e3-9860c1c548da</td>\n",
       "      <td>234382942865437071667</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Works great on HTC incredible. Can't wait to t...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2.0 build 819145-1.2.0-102</td>\n",
       "      <td>2011-05-12 18:50:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>a3b8fa06-8b8f-4f2f-a1fa-fd37c4cbf598</td>\n",
       "      <td>174473604608358796368</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Works great on nexus s</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5.2 build 389</td>\n",
       "      <td>2011-05-12 18:55:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>837fdfa5-606d-4cec-9e9a-e4a83dad633e</td>\n",
       "      <td>286593453219054880269</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Working perfect for me on EVO, running CM 7.0.3.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2.1 build 843839-1.2.0-30</td>\n",
       "      <td>2011-05-12 19:31:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>a8aaecb2-6984-44f7-b958-3f89f64d75f9</td>\n",
       "      <td>167276875678680630145</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>cm7 2.3.3 N1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5.2 build 389</td>\n",
       "      <td>2011-05-12 19:32:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                             review_id       pseudo_author_id  \\\n",
       "0           0  7e73f80e-a8fd-4ff3-b09b-502f0ad058ff  152618553977019693742   \n",
       "1           1  dab55eca-c2a0-45a8-93e3-9860c1c548da  234382942865437071667   \n",
       "2           2  a3b8fa06-8b8f-4f2f-a1fa-fd37c4cbf598  174473604608358796368   \n",
       "3           3  837fdfa5-606d-4cec-9e9a-e4a83dad633e  286593453219054880269   \n",
       "4           4  a8aaecb2-6984-44f7-b958-3f89f64d75f9  167276875678680630145   \n",
       "\n",
       "     author_name                                        review_text  \\\n",
       "0  A Google user  Works great on my Evo! Glad android phones are...   \n",
       "1  A Google user  Works great on HTC incredible. Can't wait to t...   \n",
       "2  A Google user                             Works great on nexus s   \n",
       "3  A Google user  Working perfect for me on EVO, running CM 7.0.3.1   \n",
       "4  A Google user                                       cm7 2.3.3 N1   \n",
       "\n",
       "   review_rating  review_likes            author_app_version  \\\n",
       "0              5             1  1.2.0 build 819145-1.2.0-102   \n",
       "1              5             1  1.2.0 build 819145-1.2.0-102   \n",
       "2              5             0               1.5.2 build 389   \n",
       "3              5             0   1.2.1 build 843839-1.2.0-30   \n",
       "4              5             0               1.5.2 build 389   \n",
       "\n",
       "      review_timestamp  \n",
       "0  2011-05-12 18:50:37  \n",
       "1  2011-05-12 18:50:52  \n",
       "2  2011-05-12 18:55:14  \n",
       "3  2011-05-12 19:31:46  \n",
       "4  2011-05-12 19:32:50  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/netflix_reviews/NETFLIX_REVIEWS.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1531126, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1531126 entries, 0 to 1531125\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count    Dtype \n",
      "---  ------              --------------    ----- \n",
      " 0   Unnamed: 0          1531126 non-null  int64 \n",
      " 1   review_id           1531126 non-null  object\n",
      " 2   pseudo_author_id    1531126 non-null  object\n",
      " 3   author_name         1531126 non-null  object\n",
      " 4   review_text         1526013 non-null  object\n",
      " 5   review_rating       1531126 non-null  int64 \n",
      " 6   review_likes        1531126 non-null  int64 \n",
      " 7   author_app_version  1114290 non-null  object\n",
      " 8   review_timestamp    1531126 non-null  object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 105.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fins6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fins6\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Data\n",
    "df.dropna(subset=['review_text'], inplace=True)\n",
    "df['review_timestamp'] = pd.to_datetime(df['review_timestamp'])\n",
    "\n",
    "# Text processing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['processed_text'] = df['review_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>pseudo_author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_likes</th>\n",
       "      <th>author_app_version</th>\n",
       "      <th>review_timestamp</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7e73f80e-a8fd-4ff3-b09b-502f0ad058ff</td>\n",
       "      <td>152618553977019693742</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Works great on my Evo! Glad android phones are...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2.0 build 819145-1.2.0-102</td>\n",
       "      <td>2011-05-12 18:50:37</td>\n",
       "      <td>work great evo glad android phone getting love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dab55eca-c2a0-45a8-93e3-9860c1c548da</td>\n",
       "      <td>234382942865437071667</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Works great on HTC incredible. Can't wait to t...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2.0 build 819145-1.2.0-102</td>\n",
       "      <td>2011-05-12 18:50:52</td>\n",
       "      <td>work great htc incredible cant wait try video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>a3b8fa06-8b8f-4f2f-a1fa-fd37c4cbf598</td>\n",
       "      <td>174473604608358796368</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Works great on nexus s</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5.2 build 389</td>\n",
       "      <td>2011-05-12 18:55:14</td>\n",
       "      <td>work great nexus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>837fdfa5-606d-4cec-9e9a-e4a83dad633e</td>\n",
       "      <td>286593453219054880269</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Working perfect for me on EVO, running CM 7.0.3.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2.1 build 843839-1.2.0-30</td>\n",
       "      <td>2011-05-12 19:31:46</td>\n",
       "      <td>working perfect evo running cm 7031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>a8aaecb2-6984-44f7-b958-3f89f64d75f9</td>\n",
       "      <td>167276875678680630145</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>cm7 2.3.3 N1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5.2 build 389</td>\n",
       "      <td>2011-05-12 19:32:50</td>\n",
       "      <td>cm7 233 n1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531121</th>\n",
       "      <td>1531121</td>\n",
       "      <td>5b819b4a-f49f-4012-b1cc-146b581aec6e</td>\n",
       "      <td>517084783367708002209</td>\n",
       "      <td>Az*********er</td>\n",
       "      <td>Bad app</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-15 22:34:37</td>\n",
       "      <td>bad app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531122</th>\n",
       "      <td>1531122</td>\n",
       "      <td>afe340b9-68df-4df9-8a86-7e9304e1e271</td>\n",
       "      <td>217585066694826156159</td>\n",
       "      <td>Ma***********ey</td>\n",
       "      <td>What more do you want from me tf? BRING BACK P...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.94.0 build 10 50546</td>\n",
       "      <td>2023-11-15 22:44:59</td>\n",
       "      <td>want tf bring back pokemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531123</th>\n",
       "      <td>1531123</td>\n",
       "      <td>3015ab73-75e8-4f17-8377-4757abbb8f0c</td>\n",
       "      <td>268385941811343301666</td>\n",
       "      <td>Em************ey</td>\n",
       "      <td>I will love this app</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-15 22:45:05</td>\n",
       "      <td>love app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531124</th>\n",
       "      <td>1531124</td>\n",
       "      <td>25b4b68e-a432-4f21-bf1c-68835f88b56e</td>\n",
       "      <td>259993922622854778058</td>\n",
       "      <td>****</td>\n",
       "      <td>The content is great but they keep adding more...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.94.0 build 10 50546</td>\n",
       "      <td>2023-11-15 22:48:54</td>\n",
       "      <td>content great keep adding sound effect ui cann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531125</th>\n",
       "      <td>1531125</td>\n",
       "      <td>7c8e755c-0505-4e0c-ac89-719f8f5fff50</td>\n",
       "      <td>527925027675131929486</td>\n",
       "      <td>Ra************on</td>\n",
       "      <td>I promise you on this, I WILL NOT RESUBSCRIBE ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.94.0 build 10 50546</td>\n",
       "      <td>2023-11-15 22:54:42</td>\n",
       "      <td>promise resubscribe month nothing ad time watc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1526013 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0                             review_id  \\\n",
       "0                 0  7e73f80e-a8fd-4ff3-b09b-502f0ad058ff   \n",
       "1                 1  dab55eca-c2a0-45a8-93e3-9860c1c548da   \n",
       "2                 2  a3b8fa06-8b8f-4f2f-a1fa-fd37c4cbf598   \n",
       "3                 3  837fdfa5-606d-4cec-9e9a-e4a83dad633e   \n",
       "4                 4  a8aaecb2-6984-44f7-b958-3f89f64d75f9   \n",
       "...             ...                                   ...   \n",
       "1531121     1531121  5b819b4a-f49f-4012-b1cc-146b581aec6e   \n",
       "1531122     1531122  afe340b9-68df-4df9-8a86-7e9304e1e271   \n",
       "1531123     1531123  3015ab73-75e8-4f17-8377-4757abbb8f0c   \n",
       "1531124     1531124  25b4b68e-a432-4f21-bf1c-68835f88b56e   \n",
       "1531125     1531125  7c8e755c-0505-4e0c-ac89-719f8f5fff50   \n",
       "\n",
       "              pseudo_author_id       author_name  \\\n",
       "0        152618553977019693742     A Google user   \n",
       "1        234382942865437071667     A Google user   \n",
       "2        174473604608358796368     A Google user   \n",
       "3        286593453219054880269     A Google user   \n",
       "4        167276875678680630145     A Google user   \n",
       "...                        ...               ...   \n",
       "1531121  517084783367708002209     Az*********er   \n",
       "1531122  217585066694826156159   Ma***********ey   \n",
       "1531123  268385941811343301666  Em************ey   \n",
       "1531124  259993922622854778058              ****   \n",
       "1531125  527925027675131929486  Ra************on   \n",
       "\n",
       "                                               review_text  review_rating  \\\n",
       "0        Works great on my Evo! Glad android phones are...              5   \n",
       "1        Works great on HTC incredible. Can't wait to t...              5   \n",
       "2                                   Works great on nexus s              5   \n",
       "3        Working perfect for me on EVO, running CM 7.0.3.1              5   \n",
       "4                                             cm7 2.3.3 N1              5   \n",
       "...                                                    ...            ...   \n",
       "1531121                                            Bad app              1   \n",
       "1531122  What more do you want from me tf? BRING BACK P...              2   \n",
       "1531123                               I will love this app              5   \n",
       "1531124  The content is great but they keep adding more...              2   \n",
       "1531125  I promise you on this, I WILL NOT RESUBSCRIBE ...              1   \n",
       "\n",
       "         review_likes            author_app_version    review_timestamp  \\\n",
       "0                   1  1.2.0 build 819145-1.2.0-102 2011-05-12 18:50:37   \n",
       "1                   1  1.2.0 build 819145-1.2.0-102 2011-05-12 18:50:52   \n",
       "2                   0               1.5.2 build 389 2011-05-12 18:55:14   \n",
       "3                   0   1.2.1 build 843839-1.2.0-30 2011-05-12 19:31:46   \n",
       "4                   0               1.5.2 build 389 2011-05-12 19:32:50   \n",
       "...               ...                           ...                 ...   \n",
       "1531121             0                           NaN 2023-11-15 22:34:37   \n",
       "1531122             0         8.94.0 build 10 50546 2023-11-15 22:44:59   \n",
       "1531123             0                           NaN 2023-11-15 22:45:05   \n",
       "1531124             0         8.94.0 build 10 50546 2023-11-15 22:48:54   \n",
       "1531125             1         8.94.0 build 10 50546 2023-11-15 22:54:42   \n",
       "\n",
       "                                            processed_text  \n",
       "0           work great evo glad android phone getting love  \n",
       "1            work great htc incredible cant wait try video  \n",
       "2                                         work great nexus  \n",
       "3                      working perfect evo running cm 7031  \n",
       "4                                               cm7 233 n1  \n",
       "...                                                    ...  \n",
       "1531121                                            bad app  \n",
       "1531122                         want tf bring back pokemon  \n",
       "1531123                                           love app  \n",
       "1531124  content great keep adding sound effect ui cann...  \n",
       "1531125  promise resubscribe month nothing ad time watc...  \n",
       "\n",
       "[1526013 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      0,      0, ...,     20,    111,      3],\n",
       "       [     0,      0,      0, ...,    380,     74,     38],\n",
       "       [     0,      0,      0, ...,     10,      6,    443],\n",
       "       ...,\n",
       "       [     0,      0,      0, ...,      0,      3,      1],\n",
       "       [     0,      0,      0, ...,    947,    213,     43],\n",
       "       [     0,      0,      0, ...,    419,    909, 262766]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert text to sequence\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['processed_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['processed_text'])\n",
    "data = pad_sequences(sequences, maxlen=100) # truncate / pad sequence\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentimemt Analysis Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=100))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels for sentiment analysis\n",
    "\n",
    "df['sentiment'] = df['review_rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "y = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9538/9538 [==============================] - ETA: 0s - loss: 0.2900 - accuracy: 0.8863WARNING:tensorflow:Early stopping conditioned on metric `val-loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "9538/9538 [==============================] - 1569s 164ms/step - loss: 0.2900 - accuracy: 0.8863 - val_loss: 0.2780 - val_accuracy: 0.8926\n",
      "Epoch 2/5\n",
      "9538/9538 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.9068WARNING:tensorflow:Early stopping conditioned on metric `val-loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "9538/9538 [==============================] - 1633s 171ms/step - loss: 0.2505 - accuracy: 0.9068 - val_loss: 0.2800 - val_accuracy: 0.8922\n",
      "Epoch 3/5\n",
      "2963/9538 [========>.....................] - ETA: 28:52 - loss: 0.2210 - accuracy: 0.9203"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fins6\\Documents\\ML_study\\kaggle_ML\\code\\Netflix_Review_NLP)Llama7b.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fins6/Documents/ML_study/kaggle_ML/code/Netflix_Review_NLP%29Llama7b.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train Model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fins6/Documents/ML_study/kaggle_ML/code/Netflix_Review_NLP%29Llama7b.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval-loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fins6/Documents/ML_study/kaggle_ML/code/Netflix_Review_NLP%29Llama7b.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test), epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[early_stopping])\n",
      "File \u001b[1;32mc:\\Users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\fins6\\anaconda3\\envs\\ds_study\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "early_stopping = EarlyStopping(monitor='val-loss', patience=2)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating Distribution Analysis\n",
    "df['review_rating'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.title('Distribution of Review Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "df.set_index('review_timestamp')['review_rating'].resample('M').mean().plot()\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Monthly Rating')\n",
    "plt.title('Average Monthly Rating Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=800,background_color='white', stopwords=stop_words, min_font_size=10).generate(\" \".join(df['processed_text']))\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(8,8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(apd=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of words\n",
    "word_freq = Counter(\" \".join(df['processed_text']).split()).most_common(10)\n",
    "\n",
    "# Extracting words and counts\n",
    "words, counts = zip(*word_freq)\n",
    "\n",
    "#Plotting\n",
    "plt.bar(words, counts)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Frequent Words in Reviews')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each review\n",
    "df['review_length'] = df['review_text'].apply(len)\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.hist(df['review_length'], bins=50)\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Review Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing correlation between ratings and likes\n",
    "\n",
    "correlation = df[['review_rating', 'review_likes']].corr()\n",
    "\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlatiojn between Rating and Likes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by app version and calculating average sentiment\n",
    "\n",
    "version_sentiment = df.groupby('author_app_version')['sentiment'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Grouping by app version and calculating average  sentiment\n",
    "plt.figure(figsize=(10, 5))\n",
    "version_sentiment.head(10).plot(kind='bar')\n",
    "plt.xlabel('App version')\n",
    "plt.ylabel('Average Sentiment')\n",
    "plt.title('Top 10 App Versions by Average Sentiment')\n",
    "plt.xticks(rotation=45, fontsize=8) # Reduce font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the most liked positive and negative reviews\n",
    "top_positive_review = df[df['sentiment']==1].sort_values(by='review_likes', ascending=False).iloc[0]\n",
    "top_negative_review = df[df['sentiment']==0].sort_values(by='review_likes', ascending=False).iloc[0]\n",
    "\n",
    "print(f'Top positive Review : {top_positive_review['review_text']}')\n",
    "print(f'Top negative Review : {top_negative_review['review_text']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of review likes\n",
    "plt.hist(df['review_likes'], bins=50, range=(0, 50))  # Adjust range as needed\n",
    "plt.xlabel('Number of Likes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Review Likes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of review length against sentiment\n",
    "plt.scatter(df['review_length'], df['sentiment'], alpha=0.5)\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Review Length vs Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by review_rating and calculate average sentiment for each rating\n",
    "rating_sentiment = df.groupby('review_rating')['sentiment'].mean()\n",
    "\n",
    "# Plotting\n",
    "rating_sentiment.plot(kind='bar')\n",
    "plt.xlabel('Review Rating')\n",
    "plt.ylabel('Average Sentiment')\n",
    "plt.title('Average Sentiment by Review Rating')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate n-grams\n",
    "def generate_ngrams(text, n=2):\n",
    "    words = text.split()\n",
    "    output = list(ngrams(words, n))\n",
    "    return output\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams = df['processed_text'].apply(lambda x: generate_ngrams(x,2))\n",
    "\n",
    "# Flatten the list of bigrams and count frequency\n",
    "bigram_freq = FreqDist([bigram for sublist in bigrams for bigram in sublist])\n",
    "\n",
    "# Plot the most common bigrams\n",
    "bigram_freq.plot(10, title='Top 10 Most Common Bigrams')dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your padded sequence data\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(data)\n",
    "\n",
    "# Plotting the PCA results\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('PCA of Review Text Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder\n",
    "input_dim = data.shape[1] # Assuming 'data' is preprocessed text data\n",
    "encoding_dim = 64 # Size of the encoding\n",
    "\n",
    "# Encoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Encoder model (for later use)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(data, data, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Encode the data\n",
    "encoded_data = encoder.predict(data)\n",
    "\n",
    "# Apply K-means clustering on the encoded data\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the dimensionality of the encoded data to two dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(encoded_data)\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters, cmap='viridis', alpha=0.5)\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('Clustering of Reviews (Reduced to 2D using PCA)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example review text\n",
    "text = df['review_text'][10:20]:  # You can select any review text\n",
    "\n",
    "# Apply NER\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Hugging Face API token\n",
    "auth_token = \"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\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=auth_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=auth_token)\n",
    "\n",
    "def ask_llama(question, context, max_length=150):\n",
    "    \"\"\"\n",
    "    Asks a question to the LLaMA model given some context.\n",
    "    \"\"\"\n",
    "    input_text = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate a response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_length=max_length, num_beams=5)\n",
    "\n",
    "    # Decode and return the response\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using the first review as context\n",
    "context = df['review_text'][3]  # Adjust index as needed for different reviews\n",
    "question = \"What are the main points of this review?\"\n",
    "\n",
    "answer = ask_llama(question, context)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
